# Camunda Morgan Stanley PoC Examples and Starter

The purpose of this application is to have a starter for PoC and other less structured engagements. This enables the consultant to easily discuss and demonstrate common concepts and patterns without customizing the application. Additionally it can be used as a starter project in many cases.

### Use Case Examples
|===
| **Use Case** | **Feature Examples**
| link:./docs/technical-case-events-and-messaging.adoc[Technical Case - Events and Messaging] | Integration with RabbitMQ, Global Execution Listener
| link:./docs/technical-case-camunda-variables.adoc[Technical Case - Camunda Variables] | REST, Objects, JSON, Entity Manager
| link:/docs/technical-case-history-engine-plugin.adoc[Technical Case - History Engine Plugin] | History Configuration, History Event Handler, History Transaction Listener, Custom History Level
| link:/docs/technical-case-camunda-generic-pros-hisrtory-config.adoc[Technical Case - Generic Properties] | History Configuration with Generic Properties
| link:/docs/technical-case-camunda-custom-incident-handler.adoc[Technical Case - Generic Properties] | History Configuration with Generic Properties
|===

## Running the use-case
IMPORTANT: Running the app once with no profile is necessary to initalize the Camunda database.
```
mvn spring-boot:run
```

Then again with the profiles
```
mvn spring-boot:run -Dspring.profiles.active=incident,integration
```

**Profiles** can be specified at the command line when the application starts. The notation is as follows.

`-Dspring.profiles.active=incident,integration,cors`

Or you can use the application.properties file to specify the profile.

```yaml
spring.profiles.active: incident,integration,cors
```

IMPORTANT: Using Camunda Enterprise - it's necessary to request a Camunda Enterprise License. 


### Testing with Postman

Use the postman collection `Service-Request-PoC.postman_collection.json` in the `postman` folder.

With post-man you can move through the processes simulating REST requests to the app.
- starting the process with new service request
- completing user tasks
- correlating messages

### See it running
**Visit `http://<server>:<port>/sr` to access the React app.**


## Architecture

image:./docs/images/service-request-architecture.png[Service Request BPMN]
====
The diagram above illustrates the interactions and logical components of the app. Note the app is all packed together into one artifact for easier development and PoC ing. But each component could be easily it's own deployable artifact.

The green bar signifies HazelCast. Events are published to HazelCast from the workflow Send Tasks. The Send tasks are implemented as java delegates. This pattern works well as we can utilize the Delegate and the Send task to control the execution of the workflow and potentially ack HazelCast  and handle incidents when Publishing fails.

The orange boxes signify components that sub-scribe to HazelCast topinc and update other components based on the Events that they receive.

The blue boxes signify components that do specific work and are updated by Events from the subscription components.

Purple lines and events signify point-to-point synchronous HTTP/REST interactions with the workflow and other components.

Orange lines and events signify pub/sub asynchronous interaction with the workflow and other components.

====
NOTE: This is a typical pattern for micro-service architectures though the level of abstraction between components will vary from use-case to use-case.

### HazelCast Integration
The spring-boot app is using spring cloud streams.

https://spring.io/projects/spring-cloud-stream

====
The app has a has a single publisher and a single subscriber for the service-request-events topic.

```
spring.cloud.stream.bindings.publishServiceRequest.destination=service-request-events
spring.cloud.stream.bindings.subscribeServiceRequest.destination=service-request-events
```
See the `com.camunda.poc.starter.usecase.servicerequest.HazelCast.integration` package/folder for impl of publishers and subscribers.
====

====
A single subscriber is implemented `ServiceRequestEventSubscriber.java`; it simply gets the message from the topic and serialized into memory. Then it saves/caches the Service Request into the local db based on the event type.
====

====
Two publishers are implemented, `ServiceRequestCreateEventPublishingDelegate.java` and `ServiceRequestUpdateEventPublishingDelegate.java`. These publishers are also JavaDelegtes and are wired into the bpmn model and are executed during the process execution. This is a powerful pattern as it lets us control the execution of the workflow and handle errors incidents and more.

The publisher publishes a ServiceRequestEvent with event meta-data such as the Name and Type as well as parameters such as workflow state and business data. It also encapsulates a ServiceRequest. which is our primary business object.

The Service Request Event meta-data helps other components understand what to do with the event.
====

### Handling Business Data and UI Integration
TIP: see the pattern described in the Camunda Best Practices https://camunda.com/best-practices/handling-data-in-processes/ and https://camunda.com/best-practices/enhancing-tasklists-with-business-data/

Often and for numerous reasons we need to consolidate data from different sources. In this app I use JPA and Spring REST with some of springs features to build a custom API. Primarily for making integration with the UI easier. Here are few reasons why I take this approach.

- Reduce queries the UI does to the backend
- Make it easier to build UI components
- Create abstraction layer that can be used to integrate other technical and business requirements like reporting and security.
- Have a source of truth for process meta-data

NOTE: Also keep in mind I want to keep every-thing self contained for PoC purposes. Think in logical terms and that these components could be another technology or several other technologies depending on the specific needs.

#### How does it work
====
Spring REST controllers are used to post data to the workflow. Simple and concise API's are defined for interaction with the process. See `ServiceRequestController.java` The API always takes a SerivceRequest object and returns and HTTP Status. The RESTful endpoint context mapping is associated to Commands that can happen in the system. Such as CREATE, UPDATE and SAVE and intended to work in a point-to-point(request/response) synchronous fashion.

The controllers only write POST/PUT data CREATING and UPDATING only.
====
IMPORTANT: The intention is to create a https://martinfowler.com/bliki/CQRS.html#:~:text=CQRS%20stands%20for%20Command%20Query,you%20use%20to%20read%20information[CQRS(Command Query Responsibility Segregation)] pattern for interaction. This can increase scalability while reducing complexity in distributed systems.
====
A separate API and logically separate data-store is used to query ServiceRequest data. Updates to this data-store always happen in an asynchronous fashion. For example when a Approval task in the work flow completes the workflow publishes an UPDATE-SERVICE-REQUEST event. The subscriber reads the event and decides what to do. In certain cases it updates the ServiceRequest in the data-store. Other components can now read from the data-store such as the UI.

We can guarantee the data is published to the data store with the workflow. See the section above on the JavaDelegates that implement the publishing functionality.

https://spring.io/blog/2011/02/10/getting-started-with-spring-data-jpa[Spring Data JPA] is the technology used for the ServiceRequest data. Spring Data allows for an easy way to create API's that are easy for a UI to query. Also an easy way to combine data into a useful form for the UI to consume.
====

## Developing with this PoC Starter Project
#### Setting up React for Dev

- Configure the api endpoint. This is the backend spring-boot server where the react app gets data
```
In the .env file in the project home directory change the environment variables to match the spring-boot server context.

* Note you should only need to do this if you cannot access the spring-boot server on localhost and you plan to run the React App standalone.

* If running the react app as a standalalone and not on localhost configure the API_HOST and API_POST environment vars as follows inserting your host and port for the spring-boot server.

    API_HOST=http://127.0.0.1
    API_PORT=8080
    API_ROOT=api

* Note, you will need to use the cors profile in this setup and potentially modify the cors config in the spring-boot app.

```
- Run node and server.js by starting a node server in the home directory of the project. You may need to run `npm install` first.
```
nodemon server.js
```
also run the web-pack watch in the project home so you can update the bundle as you build reactjs
```
webpack -w
```

#### Running the server for Dev
NOTE: you need to run the cors profile when using nodemon

- Also note you can use spring-dev-tools to build front and back-end component in dev mode providing faster restarts and live-reload.

for dev mode run the following with the appropriate profiles
```
mvn spring-boot:run mvn spring-boot:run -Dspring.profiles.active=servicerequest,integration,cors
```

WARNING: spring-dev-tools affects the way Camunda serializes objects into process vars and will cause serialization errors in some cases. So it is commented out in pom.xml by default.

#### Running HazelCast
NOTE: A simple HazelCast congfig is packaged into docker-compose. See `docker-compose` directory in the project home folder. Also you can run docker compose as follows.
```
 docker-compose up
```
HazelCast image docs https://hub.docker.com/r/hazelcast/hazelcast

HazelCast getting started https://hazelcast.org/imdg/get-started/

HazelCast Architecture Overview


### Externalizing Configuration

All the properties in the .properties files can be externalized.

Look at the **_application-prod.properties_** files for examples of utilizing environment variables with the app. This is particularly useful if your running in serverless environment and you cannot store property values in your code repo.


## Deploying the app

### Docker and Docker-Compose

Services are configure in the docker-compose.yaml to run the app in docker with a default network.

1. Uncomment the appropriate services for your application and profiles in the docker-compose.yaml located in the docker-compose directory.
2. Make sure to build the spring-boot app
3. run docker-compose up from the docker-compose directory


#### Environment configs

There are a few configurations you must add to Heroku for the app to work correctly

- JDBC_DATABASE_URL - This config is added for you. You wont see it in the Heroku dashboard. Run the following command to confirm this is set.

